Current neural network idea that if you want to be able to do something with a quince and started to augment. Currency of a feedback propagation time. We also recognized some of the limitations back propagation through time the algorithm a was acceptable to voting for radius And also make a prediction of times he is kind of confusing information on x t - 1. So on so forth, so it was played by what was the own as short-term memory. Do events at the Academy evolution of that. I was a long short term memory. And they udid. And by doing so they can. for a little bit longer German film they still have an improvement over R & M Silver Plate still have issues. Idea of Transformers with a W and all these large language moderns are built off of a cell phone as my Slammer here. I want to take a look at the architecture if it's a little bit about what's happening in a Transformer because I certainly is highly related to learning art of natural language. 815 South 400 MW if you are there is a course in the data science course number 691 and offered quite often and pretty technical courses offered for the Austin as well. And ultimately to do a lot of stuff is happening in Transformers. He's had a lot of baccarat background in NLP nevertheless. I think we can kind of still talk about you. Large lighted models and Transformers all kind of date. important paper on attention is all you need and they're the authors propose that rather than observation at a time in a sequence better to provide the entire sequence at once and allow the system to learn what is important in that sentence to make me so it's a lot better contact for what was pretty intense this idea of attention. You give it a politically is going to figure out where to put attention in order to figure out. And it's kind of hard to text you that they created they referred to as transform. Then it's kind of a visualization from their paper of what the architecture of a transformer looks like. It has got two sides to it one side is referred to as the encoders over here and over on the right hand side. You have to prefer to smile. Encoders and and after we can do the decoding, there's typically Springs hours. things that we want to predict the number of wild Transformers are using a lot of different domains that say the number of words. That we want to be able to predict the probability of essentially the next works. And started understanding what's happening is to transform start off with how are they are as also how they are used and then we'll look at that propagate. So the idea is that we start off with some input. I probably at the  